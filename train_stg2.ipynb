{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import vgg19\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'])\n",
    "\n",
    "\n",
    "def calculate_valid_crop_size(crop_size, upscale_factor):\n",
    "    return crop_size - (crop_size % upscale_factor)\n",
    "\n",
    "\n",
    "def train_hr_transform(crop_size):\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomCrop(crop_size, pad_if_needed=True),\n",
    "        transforms.RandomVerticalFlip(p=0.3),\n",
    "        transforms.RandomHorizontalFlip(p=0.4),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "\n",
    "def train_lr_transform(crop_size, upscale_factor):\n",
    "    return transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(crop_size // upscale_factor, interpolation=Image.Resampling.BICUBIC),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, dataset_dir, crop_size, upscale_factor):\n",
    "        super(TrainDataset, self).__init__()\n",
    "        self.image_filenames = []\n",
    "        for ds_path in dataset_dir:\n",
    "            for x in os.listdir(ds_path):\n",
    "                self.image_filenames.append(os.path.join(ds_path, x))\n",
    "        crop_size = calculate_valid_crop_size(crop_size, upscale_factor)\n",
    "        self.hr_transform = train_hr_transform(crop_size)\n",
    "        self.lr_transform = train_lr_transform(crop_size, upscale_factor)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        hr_image = self.hr_transform(Image.open(self.image_filenames[index]).convert('RGB'))\n",
    "        lr_image = self.lr_transform(hr_image)\n",
    "        return lr_image, hr_image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "\n",
    "class ValDataset(Dataset):\n",
    "    def __init__(self, dataset_dir, crop_size, upscale_factor):\n",
    "        super(ValDataset, self).__init__()\n",
    "        self.image_filenames = []\n",
    "        for ds_path in dataset_dir:\n",
    "            for x in os.listdir(ds_path):\n",
    "                self.image_filenames.append(os.path.join(ds_path, x))\n",
    "        self.crop_size = calculate_valid_crop_size(crop_size, upscale_factor)\n",
    "        self.upscale_factor = upscale_factor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        hr_image = Image.open(self.image_filenames[index]).convert('RGB')\n",
    "        \n",
    "        lr_scale = transforms.Resize(self.crop_size // self.upscale_factor, interpolation=Image.Resampling.BICUBIC)\n",
    "        hr_scale = transforms.Resize(self.crop_size, interpolation=Image.Resampling.BICUBIC)\n",
    "        hr_image = transforms.CenterCrop(self.crop_size)(hr_image)\n",
    "        lr_image = lr_scale(hr_image)\n",
    "        hr_restore_img = hr_scale(lr_image)\n",
    "        return to_tensor(lr_image), to_tensor(hr_restore_img), to_tensor(hr_image)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 3,\n",
    "        features: tuple = (64, 64, 128, 128, 256, 256, 512, 512),\n",
    "    ) -> None:\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        blocks = []\n",
    "        for idx, feature in enumerate(features):\n",
    "            blocks.append(\n",
    "                ConvBlock(\n",
    "                    in_channels,\n",
    "                    feature,\n",
    "                    kernel_size=3,\n",
    "                    stride=1 + idx % 2,\n",
    "                    padding=1,\n",
    "                    discriminator=True,\n",
    "                    use_act=True,\n",
    "                    use_bn=False if idx == 0 else True,\n",
    "                )\n",
    "            )\n",
    "            in_channels = feature\n",
    "\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((6, 6)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 6 * 6, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.blocks(x)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "class GeneratorLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GeneratorLoss, self).__init__()\n",
    "        vgg = vgg19(pretrained=True)\n",
    "        loss_network = nn.Sequential(*list(vgg.features)).eval()\n",
    "        for param in loss_network.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.loss_network = loss_network\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.tv_loss = TVLoss()\n",
    "        self.preprocess = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def forward(self, fake_out, real_out, out_images, target_images):\n",
    "        # Adversarial Loss\n",
    "        adversarial_loss = nn.BCEWithLogitsLoss()(fake_out - real_out, target_real)\n",
    "        # Perception Loss\n",
    "        a = self.preprocess(out_images)\n",
    "        b = self.preprocess(target_images)\n",
    "        perception_loss = self.mse_loss(self.loss_network(a), self.loss_network(b))\n",
    "        # Image Loss\n",
    "        # image_loss = self.mse_loss(out_images, target_images)\n",
    "        # TV Loss\n",
    "        tv_loss = self.tv_loss(out_images)\n",
    "        return 0.6 * adversarial_loss + perception_loss + 1e-6 * tv_loss\n",
    "\n",
    "\n",
    "class TVLoss(nn.Module):\n",
    "    def __init__(self, tv_loss_weight=1):\n",
    "        super(TVLoss, self).__init__()\n",
    "        self.tv_loss_weight = tv_loss_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size()[0]\n",
    "        h_x = x.size()[2]\n",
    "        w_x = x.size()[3]\n",
    "        count_h = self.tensor_size(x[:, :, 1:, :])\n",
    "        count_w = self.tensor_size(x[:, :, :, 1:])\n",
    "        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()\n",
    "        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()\n",
    "        return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n",
    "\n",
    "    @staticmethod\n",
    "    def tensor_size(t):\n",
    "        return t.size()[1] * t.size()[2] * t.size()[3]\n",
    "\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([math.exp(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2)) for x in range(window_size)])\n",
    "    return gauss / gauss.sum()\n",
    "\n",
    "\n",
    "def create_window(window_size, channel):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
    "    return window\n",
    "\n",
    "\n",
    "def _ssim(img1, img2, window, window_size, channel, size_average=True):\n",
    "    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2\n",
    "\n",
    "    C1 = 0.01 ** 2\n",
    "    C2 = 0.03 ** 2\n",
    "\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    if size_average:\n",
    "        return ssim_map.mean()\n",
    "    else:\n",
    "        return ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "\n",
    "class SSIM(torch.nn.Module):\n",
    "    def __init__(self, window_size=11, size_average=True):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "        self.channel = 1\n",
    "        self.window = create_window(window_size, self.channel)\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        (_, channel, _, _) = img1.size()\n",
    "\n",
    "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
    "            window = self.window\n",
    "        else:\n",
    "            window = create_window(self.window_size, channel)\n",
    "\n",
    "            if img1.is_cuda:\n",
    "                window = window.cuda(img1.get_device())\n",
    "            window = window.type_as(img1)\n",
    "\n",
    "            self.window = window\n",
    "            self.channel = channel\n",
    "\n",
    "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
    "\n",
    "\n",
    "def ssim(img1, img2, window_size=11, size_average=True):\n",
    "    (_, channel, _, _) = img1.size()\n",
    "    window = create_window(window_size, channel)\n",
    "\n",
    "    if img1.is_cuda:\n",
    "        window = window.cuda(img1.get_device())\n",
    "    window = window.type_as(img1)\n",
    "\n",
    "    return _ssim(img1, img2, window, window_size, channel, size_average)\n",
    "\n",
    "CROP_SIZE = 96\n",
    "UPSCALE_FACTOR = 4\n",
    "NUM_EPOCHS = 200\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "train_path = [\"../input/df2k-ost/train/DIV2K/DIV2K_train_HR\", \"../input/df2k-ost/train/Flickr2K\", \"../input/df2k-ost/train/OST\"]\n",
    "test_path = [\"../input/df2k-ost/test/DIV2K_valid\"]\n",
    "\n",
    "train_set = TrainDataset(train_path, crop_size=CROP_SIZE, upscale_factor=UPSCALE_FACTOR)\n",
    "val_set = ValDataset(test_path, crop_size=CROP_SIZE, upscale_factor=UPSCALE_FACTOR)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_set,\n",
    "    num_workers=2,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "val_loader = DataLoader(dataset=val_set, num_workers=2, batch_size=1, shuffle=False)\n",
    "\n",
    "netG = Generator(upscale_factor=UPSCALE_FACTOR).to(DEVICE)\n",
    "print(\"# generator parameters:\", sum(param.numel() for param in netG.parameters()))\n",
    "netG.load_state_dict(torch.load(\"../input/finalmse/netG_4x_epoch200.pth.tar\")['model'])\n",
    "\n",
    "netD = Discriminator().to(DEVICE)\n",
    "print(\n",
    "    \"# discriminator parameters:\", sum(param.numel() for param in netD.parameters())\n",
    ")\n",
    "\n",
    "generator_criterion = GeneratorLoss().to(DEVICE)\n",
    "\n",
    "optimizerG = torch.optim.AdamW(netG.parameters(), lr=1e-5)\n",
    "optimizerD = torch.optim.AdamW(netD.parameters(), lr=1e-5)\n",
    "\n",
    "scheduler_G = lr_scheduler.StepLR(optimizerG, step_size=100, gamma=0.5)\n",
    "scheduler_D = lr_scheduler.StepLR(optimizerD, step_size=100, gamma=0.5)\n",
    "\n",
    "results = {\n",
    "    \"d_loss\": [],\n",
    "    \"g_loss\": [],\n",
    "    \"psnr\": [],\n",
    "    \"ssim\": []\n",
    "}\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    scheduler_G.step()\n",
    "    scheduler_D.step()\n",
    "    cur_lr = optimizerG.param_groups[0]['lr']\n",
    "    train_bar = tqdm(train_loader, total=len(train_loader))\n",
    "    running_results = {\n",
    "        \"batch_sizes\": 0,\n",
    "        \"d_loss\": 0,\n",
    "        \"g_loss\": 0,\n",
    "        \"learning_rate\": cur_lr\n",
    "    }\n",
    "\n",
    "    netG.train()\n",
    "    netD.train()\n",
    "    for lr_img, hr_img in train_bar:\n",
    "        batch_size = lr_img.size(0)\n",
    "        running_results[\"batch_sizes\"] += batch_size\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize D(x)-1-D(G(z))\n",
    "        ###########################\n",
    "        hr_img = hr_img.to(DEVICE)\n",
    "        lr_img = lr_img.to(DEVICE)\n",
    "\n",
    "        sr_img = netG(lr_img)\n",
    "\n",
    "        netD.zero_grad()\n",
    "        real_out = netD(hr_img)\n",
    "        fake_out = netD(sr_img)\n",
    "        target_real = torch.Tensor(batch_size, 1).fill_(1.0).to(DEVICE)\n",
    "        target_fake = torch.Tensor(batch_size, 1).fill_(0.0).to(DEVICE)\n",
    "\n",
    "        d_loss = nn.BCEWithLogitsLoss()(real_out - fake_out, target_real)\n",
    "        d_loss.backward(retain_graph=True)\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: minimize 1-D(G(z)) + Perception Loss + Image Loss + TV Loss\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "\n",
    "        sr_img = netG(lr_img)\n",
    "        fake_out = netD(sr_img)\n",
    "        real_out = netD(hr_img)\n",
    "\n",
    "        g_loss = generator_criterion(fake_out, real_out, sr_img, hr_img)\n",
    "        g_loss.backward()\n",
    "\n",
    "        optimizerG.step()\n",
    "\n",
    "        # loss for current after before optimization\n",
    "        running_results[\"g_loss\"] += g_loss.item() * batch_size\n",
    "        running_results[\"d_loss\"] += d_loss.item() * batch_size\n",
    "\n",
    "        train_bar.set_description(\n",
    "            desc=\"[%d/%d] Loss_D: %f Loss_G: %f Learning_rate: %f\"\n",
    "            % (\n",
    "                epoch,\n",
    "                NUM_EPOCHS,\n",
    "                running_results[\"d_loss\"] / running_results[\"batch_sizes\"],\n",
    "                running_results[\"g_loss\"] / running_results[\"batch_sizes\"],\n",
    "                running_results[\"learning_rate\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    netG.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_bar = tqdm(val_loader, total=len(val_loader))\n",
    "        valing_results = {\n",
    "            \"mse\": 0,\n",
    "            \"ssims\": 0,\n",
    "            \"psnr\": 0,\n",
    "            \"ssim\": 0,\n",
    "            \"batch_sizes\": 0,\n",
    "        }\n",
    "        val_images = []\n",
    "        for val_lr, val_hr in val_bar:\n",
    "            batch_size = val_lr.size(0)\n",
    "            valing_results[\"batch_sizes\"] += batch_size\n",
    "            lr = val_lr\n",
    "            hr = val_hr\n",
    "            if torch.cuda.is_available():\n",
    "                lr = lr.cuda()\n",
    "                hr = hr.cuda()\n",
    "            # Forward\n",
    "            sr = netG(lr)\n",
    "            # Loss & metrics\n",
    "            batch_mse = ((sr - hr) ** 2).data.mean()\n",
    "            valing_results[\"mse\"] += batch_mse * batch_size\n",
    "            batch_ssim = ssim(sr, hr).item()\n",
    "\n",
    "            valing_results[\"ssims\"] += batch_ssim * batch_size\n",
    "            valing_results[\"psnr\"] = 10 * math.log10(\n",
    "                (hr.max() ** 2)\n",
    "                / (valing_results[\"mse\"] / valing_results[\"batch_sizes\"])\n",
    "            )\n",
    "            valing_results[\"ssim\"] = (\n",
    "                valing_results[\"ssims\"] / valing_results[\"batch_sizes\"]\n",
    "            )\n",
    "            val_bar.set_description(\n",
    "                desc=\"[converting LR images to SR images] PSNR: %.4f dB SSIM: %.4f\"\n",
    "                % (valing_results[\"psnr\"], valing_results[\"ssim\"])\n",
    "            )\n",
    "\n",
    "    # save model parameters\n",
    "    netG.train()\n",
    "    netD.train()\n",
    "    \n",
    "    #########################\n",
    "    torch.save(\n",
    "        {\"model\": netG.state_dict()},\n",
    "        f\"./netG_{UPSCALE_FACTOR}x_epoch{epoch}.pth.tar\",\n",
    "    )\n",
    "    torch.save(\n",
    "        {\"model\": netD.state_dict()},\n",
    "        f\"./netD_{UPSCALE_FACTOR}x_epoch{epoch}.pth.tar\",\n",
    "    )\n",
    "    #########################\n",
    "\n",
    "    results[\"d_loss\"].append(\n",
    "        running_results[\"d_loss\"] / running_results[\"batch_sizes\"]\n",
    "    )\n",
    "    results[\"g_loss\"].append(\n",
    "        running_results[\"g_loss\"] / running_results[\"batch_sizes\"]\n",
    "    )\n",
    "\n",
    "    results[\"psnr\"].append(valing_results[\"psnr\"])\n",
    "    results[\"ssim\"].append(valing_results[\"ssim\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class SeperableConv2d(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, bias=True, padding_mode='zeros'):\n",
    "        super(SeperableConv2d, self).__init__(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels, out_channels=in_channels, groups=in_channels, \n",
    "                kernel_size=kernel_size, padding='same', dilation=dilation,\n",
    "                bias=bias, padding_mode=padding_mode\n",
    "            ),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "\n",
    "class UpsampleBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels, scale_factor):\n",
    "        super(UpsampleBlock, self).__init__(\n",
    "            SeperableConv2d(in_channels, in_channels * scale_factor**2, kernel_size=3),\n",
    "            nn.PixelShuffle(scale_factor),\n",
    "            nn.PReLU(num_parameters=in_channels),\n",
    "            SeperableConv2d(in_channels, in_channels * scale_factor**2, kernel_size=3),\n",
    "            nn.PixelShuffle(scale_factor),\n",
    "            nn.PReLU(num_parameters=in_channels)\n",
    "        )\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, use_bn=False, use_ffc=False, use_act=True, discriminator=False, **kwargs):\n",
    "        if use_ffc: conv = FFC(in_channels, out_channels, kernel_size=3, \n",
    "                ratio_gin=0.5, ratio_gout=0.5, inline = True\n",
    "            )\n",
    "        else: conv = SeperableConv2d(in_channels, out_channels, **kwargs)\n",
    "        m = [conv]\n",
    "        \n",
    "        if use_bn: m.append(nn.BatchNorm2d(out_channels))\n",
    "        if use_act: m.append(nn.LeakyReLU(0.2, inplace=True) if discriminator else nn.PReLU(num_parameters=out_channels))\n",
    "        super(ConvBlock, self).__init__(*m)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, index):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.block1 = ConvBlock(\n",
    "            in_channels,\n",
    "            in_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            use_ffc=True if index % 2 == 0 else False\n",
    "        )\n",
    "        self.block2 = ConvBlock(\n",
    "            in_channels,\n",
    "            in_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            use_act=False\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.block1(x)\n",
    "        out = self.block2(out)\n",
    "        out = out.mul(0.1)\n",
    "        out += x\n",
    "        return out\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels: int = 3, num_channels: int = 64, num_blocks: int = 16, upscale_factor: int = 4):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.initial = ConvBlock(in_channels, num_channels, kernel_size=3, use_act=False)\n",
    "        self.residual = nn.Sequential(\n",
    "            *[ResidualBlock(num_channels, _) for _ in range(num_blocks)]\n",
    "        )\n",
    "        self.upsampler = UpsampleBlock(num_channels, scale_factor=2)\n",
    "        self.final_conv = SeperableConv2d(num_channels, in_channels, kernel_size=3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        initial = self.initial(x)\n",
    "        x = self.residual(initial) + initial\n",
    "        x = self.upsampler(x)\n",
    "        out = self.final_conv(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FourierUnit(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, ffc3d=False, fft_norm='ortho'):\n",
    "        super(FourierUnit, self).__init__()\n",
    "        self.conv_layer = SeperableConv2d(in_channels=in_channels * 2,\n",
    "                                          out_channels=out_channels * 2,\n",
    "                                          kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "\n",
    "        self.ffc3d = ffc3d\n",
    "        self.fft_norm = fft_norm\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch = x.shape[0]\n",
    "        r_size = x.size()\n",
    "\n",
    "        # (batch, c, h, w/2+1, 2)\n",
    "        fft_dim = (-3, -2, -1) if self.ffc3d else (-2, -1)\n",
    "        ffted = torch.fft.rfftn(x, dim=fft_dim, norm=self.fft_norm)\n",
    "        ffted = torch.stack((ffted.real, ffted.imag), dim=-1)\n",
    "        ffted = ffted.permute(0, 1, 4, 2, 3).contiguous()  # (batch, c, 2, h, w/2+1)\n",
    "        ffted = ffted.view((batch, -1,) + ffted.size()[3:])\n",
    "\n",
    "        ffted = self.conv_layer(ffted)  # (batch, c*2, h, w/2+1)\n",
    "        ffted = self.relu(ffted)\n",
    "\n",
    "        ffted = ffted.view((batch, -1, 2,) + ffted.size()[2:]).permute(\n",
    "            0, 1, 3, 4, 2).contiguous()  # (batch,c, t, h, w/2+1, 2)\n",
    "        ffted = torch.complex(ffted[..., 0], ffted[..., 1])\n",
    "\n",
    "        ifft_shape_slice = x.shape[-3:] if self.ffc3d else x.shape[-2:]\n",
    "        output = torch.fft.irfftn(ffted, s=ifft_shape_slice, dim=fft_dim, norm=self.fft_norm)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class SpectralTransform(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, enable_lfu=True, **fu_kwargs):\n",
    "        super(SpectralTransform, self).__init__()\n",
    "        self.enable_lfu = enable_lfu\n",
    "        if stride == 2:\n",
    "            self.downsample = nn.AvgPool2d(kernel_size=(2, 2), stride=2)\n",
    "        else:\n",
    "            self.downsample = nn.Identity()\n",
    "\n",
    "        self.stride = stride\n",
    "        self.conv1 = nn.Sequential(\n",
    "            SeperableConv2d(in_channels, out_channels // 2, kernel_size=1, bias=False),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.fu = FourierUnit(\n",
    "            out_channels // 2, out_channels // 2, **fu_kwargs)\n",
    "\n",
    "        if self.enable_lfu:\n",
    "            self.lfu = FourierUnit(out_channels // 2, out_channels // 2)\n",
    "        self.conv2 = SeperableConv2d(out_channels // 2, out_channels, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.downsample(x)\n",
    "        x = self.conv1(x)\n",
    "        output = self.fu(x)\n",
    "        \n",
    "        if self.enable_lfu:\n",
    "            n, c, h, w = x.shape\n",
    "            split_no = 2\n",
    "            split_h = h // split_no\n",
    "            split_w = w // split_no\n",
    "            xs = torch.cat(torch.split(x[:, :c // 4], split_h, dim=-2)[0:2], dim=1).contiguous()\n",
    "            xs = torch.cat(torch.split(xs, split_w, dim=-1)[0:2], dim=1).contiguous()\n",
    "            xs = self.lfu(xs)\n",
    "            xs = xs.repeat(1, 1, split_no, split_no).contiguous()\n",
    "\n",
    "            if h % 2 == 1:\n",
    "                h_zeros = torch.zeros(xs.shape[0], xs.shape[1], 1, xs.shape[3]).to(DEVICE)\n",
    "                xs = torch.cat((xs, h_zeros), dim=2)\n",
    "            if w % 2 == 1:\n",
    "                w_zeros = torch.zeros(xs.shape[0], xs.shape[1], xs.shape[2], 1).to(DEVICE)\n",
    "                xs = torch.cat((xs, w_zeros), dim=3)\n",
    "        else:\n",
    "            xs = 0\n",
    "\n",
    "        output = self.conv2(x + output + xs)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class FFC(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 ratio_gin, ratio_gout, inline=True, stride=1, padding=0,\n",
    "                 dilation=1, enable_lfu=True,\n",
    "                 padding_type='reflect', gated=False, **spectral_kwargs):\n",
    "        super(FFC, self).__init__()\n",
    "\n",
    "        assert stride == 1 or stride == 2, \"Stride should be 1 or 2.\"\n",
    "        self.stride = stride\n",
    "        self.inline = inline\n",
    "\n",
    "        in_cg = int(in_channels * ratio_gin)\n",
    "        in_cl = in_channels - in_cg\n",
    "        out_cg = int(out_channels * ratio_gout)\n",
    "        out_cl = out_channels - out_cg\n",
    "\n",
    "        self.ratio_gin = ratio_gin\n",
    "        self.ratio_gout = ratio_gout\n",
    "        self.global_in_num = in_cg\n",
    "\n",
    "        module = nn.Identity if in_cl == 0 or out_cl == 0 else SeperableConv2d\n",
    "        self.convl2l = module(in_cl, out_cl, kernel_size,\n",
    "                              stride, padding, dilation, padding_mode=padding_type)\n",
    "        module = nn.Identity if in_cl == 0 or out_cg == 0 else SeperableConv2d\n",
    "        self.convl2g = module(in_cl, out_cg, kernel_size,\n",
    "                              stride, padding, dilation, padding_mode=padding_type)\n",
    "        module = nn.Identity if in_cg == 0 or out_cl == 0 else SeperableConv2d\n",
    "        self.convg2l = module(in_cg, out_cl, kernel_size,\n",
    "                              stride, padding, dilation, padding_mode=padding_type)\n",
    "        module = nn.Identity if in_cg == 0 or out_cg == 0 else SpectralTransform\n",
    "        self.convg2g = module(\n",
    "            in_cg, out_cg, stride, enable_lfu, **spectral_kwargs)\n",
    "\n",
    "        self.gated = gated\n",
    "        module = nn.Identity if in_cg == 0 or out_cl == 0 or not self.gated else SeperableConv2d\n",
    "        self.gate = module(in_channels, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.inline:\n",
    "            x_l, x_g = x[:, :-self.global_in_num], x[:, -self.global_in_num:]\n",
    "        else:\n",
    "            x_l, x_g = x if type(x) is tuple else (x, 0)\n",
    "        out_xl, out_xg = 0, 0\n",
    "\n",
    "        if self.gated:\n",
    "            total_input_parts = [x_l]\n",
    "            if torch.is_tensor(x_g):\n",
    "                total_input_parts.append(x_g)\n",
    "            total_input = torch.cat(total_input_parts, dim=1)\n",
    "\n",
    "            gates = torch.sigmoid(self.gate(total_input))\n",
    "            g2l_gate, l2g_gate = gates.chunk(2, dim=1)\n",
    "        else:\n",
    "            g2l_gate, l2g_gate = 1, 1\n",
    "\n",
    "        if self.ratio_gout != 1:\n",
    "            out_xl = self.convl2l(x_l) + self.convg2l(x_g) * g2l_gate\n",
    "        if self.ratio_gout != 0:\n",
    "            out_xg = self.convl2g(x_l) * l2g_gate + self.convg2g(x_g)\n",
    "            \n",
    "        out = out_xl, out_xg\n",
    "        if self.inline:\n",
    "            out = torch.cat(out, dim=1)\n",
    "\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
